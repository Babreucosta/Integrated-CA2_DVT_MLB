{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd44276",
   "metadata": {},
   "source": [
    "<div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:arial;text-align:center;display:fill;border-radius:5px; letter-spacing: 2px; background-color:#257CCB;overflow:hidden\"><b>INTEGRATED CONTINUOUS ASSESSMENT</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5be77",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "<h2 style=\"font-family:arial;text-align:center; font-size: 32px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: centre; letter-spacing: 5px; color:#257CCB; background-color: #ffffff;\"> <b>DATA VISUALISATION + MACHINE LEARNING</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35284fbd",
   "metadata": {},
   "source": [
    "Programme Title: HDIP Data Analytics\n",
    "\n",
    "Cohort: HDIP Feb 22 FT/PT\n",
    "\n",
    "Module Title(s): Data Visualization Techniques & Machine Learning for Business \n",
    "\n",
    "Assignment Type: Individual  \n",
    "\n",
    "Assignment Title: Integrated CA2_DVT_MLB\n",
    "\n",
    "Lecturer(s): David McQuaid & Dr. Muhammad Iqba\n",
    "\n",
    "Issue Date: 19th October 2023  \n",
    "\n",
    "Submission Deadline Date:  26th November 2023 \n",
    "\n",
    "Student: **_Bárbara Abreu Costa 2023099_**\n",
    "\n",
    "GitHub Repository: https://github.com/Babreucosta/Integrated-CA2_DVT_MLB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab5f8e",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd9868",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <p style=\"padding:10px;background-color:#257CCB;margin:0;color:white;font-family:arial;font-size:100%;text-align:center;border-radius:5px;overflow:hidden;font-weight:500\">_MACHINE LEARNING_</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c57c5",
   "metadata": {},
   "source": [
    "**QUESTION 1**\n",
    "\n",
    "Discuss and explain the purpose of a recommendation system for online retail business in machine\n",
    "learning. Briefly compare Content and Collaborative filtering using any dataset of your choice (Datasets used\n",
    "in the class tutorials or exercises are not allowed to use in this CA2). Train and test machine learning models\n",
    "for the user-user or item-item collaborative filtering. Justify your recommendations for the considered\n",
    "scenario by providing a conceptual insight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985e3df",
   "metadata": {},
   "source": [
    "> **_RECOMMENDATION SYSTEM PURPOSE_**\n",
    "> \n",
    "> Recommendation systems play a pivotal role in the success of online retail businesses by leveraging machine learning algorithms to deliver personalized product suggestions to users. It analyze a user's historical behavior, preferences, and interactions with the platform to offer product suggestions tailored to their individual tastes and needs. This personalization creates a more engaging and relevant shopping experience. \n",
    "> \n",
    "> One of the primary purposes of a recommendation system is to augment the customer experience. By presenting relevant products to users based on their past interactions, the system assists in narrowing down choices from a vast array of offerings. This not only saves time for the customer but also creates a sense of personalization, fostering customer loyalty. For instance, Amazon's recommendation engine analyzes user behavior and provides suggestions such as \"Customers who bought this also bought...\" or \"Recommended for you\", contributing significantly to their sales revenue (Linden et al., 2003).\n",
    ">\n",
    "> Another vital purpose of recommendation systems is to facilitate cross-selling and up-selling opportunities. Cross-selling involves suggesting related or complementary products to the user's current selection, while up-selling involves recommending higher-value alternatives. \n",
    ">\n",
    "> In conclusion, recommendation systems are indispensable tools for online retail businesses, leveraging machine learning techniques to enhance the customer experience, increase sales, alleviate information overload, and drive cross-selling and up-selling initiatives. Through personalized suggestions, these systems create a win-win situation, benefiting both the customer and the retailer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c796f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f3a46",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "<h2 style=\"font-family:arial;text-align:center; font-size: 32px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: centre; letter-spacing: 3px; color:#257CCB; background-color: #ffffff;\"> <b>AMAZON SALES DATASET</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99189a1b",
   "metadata": {},
   "source": [
    "This dataset is having the data of 1K+ Amazon Product's Ratings and Reviews as per their details listed on the official website of Amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc7aca",
   "metadata": {},
   "source": [
    "##### DICTIONARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e52c1e",
   "metadata": {},
   "source": [
    "| STT | ATTRIBUTE NAME | MEANING |\n",
    "|:---| :--- |:--- |\n",
    "| 0 | product_id | Product ID\n",
    "| 1 |product_name | Name of the Product\n",
    "| 2 |category | Category of the Product\n",
    "| 3 |discounted_price | Discounted Price of the Product\n",
    "| 4 |actual_price | Actual Price of the Product\n",
    "| 5 |discount_percentage | Percentage of Discount for the Product\n",
    "| 6 |rating | Rating of the Product\n",
    "| 7 |rating_count | Number of people who voted for the Amazon rating\n",
    "| 8 |about_product | Description about the Product\n",
    "| 9 |user_id | ID of the user who wrote review for the Product\n",
    "| 10 |user_name | Name of the user who wrote review for the Product\n",
    "| 11 |review_id | ID of the user review\n",
    "| 12 |review_title | Short review\n",
    "| 13 |review_content | Long review\n",
    "| 14 |img_link | Image Link of the Product\n",
    "| 15 |product_link | Official Website Link of the Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ef68c",
   "metadata": {},
   "source": [
    "##### DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84da413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa4d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('customer_shopping_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53a3e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_no</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>category</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "      <th>payment_method</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>shopping_mall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I138884</td>\n",
       "      <td>C241288</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>5</td>\n",
       "      <td>1500.40</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>5/8/2022</td>\n",
       "      <td>Kanyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I317333</td>\n",
       "      <td>C111565</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>3</td>\n",
       "      <td>1800.51</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>12/12/2021</td>\n",
       "      <td>Forum Istanbul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I127801</td>\n",
       "      <td>C266599</td>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>1</td>\n",
       "      <td>300.08</td>\n",
       "      <td>Cash</td>\n",
       "      <td>9/11/2021</td>\n",
       "      <td>Metrocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I173702</td>\n",
       "      <td>C988172</td>\n",
       "      <td>Female</td>\n",
       "      <td>66</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>5</td>\n",
       "      <td>3000.85</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>16/05/2021</td>\n",
       "      <td>Metropol AVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I337046</td>\n",
       "      <td>C189076</td>\n",
       "      <td>Female</td>\n",
       "      <td>53</td>\n",
       "      <td>Books</td>\n",
       "      <td>4</td>\n",
       "      <td>60.60</td>\n",
       "      <td>Cash</td>\n",
       "      <td>24/10/2021</td>\n",
       "      <td>Kanyon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  invoice_no customer_id  gender  age  category  quantity    price  \\\n",
       "0    I138884     C241288  Female   28  Clothing         5  1500.40   \n",
       "1    I317333     C111565    Male   21     Shoes         3  1800.51   \n",
       "2    I127801     C266599    Male   20  Clothing         1   300.08   \n",
       "3    I173702     C988172  Female   66     Shoes         5  3000.85   \n",
       "4    I337046     C189076  Female   53     Books         4    60.60   \n",
       "\n",
       "  payment_method invoice_date   shopping_mall  \n",
       "0    Credit Card     5/8/2022          Kanyon  \n",
       "1     Debit Card   12/12/2021  Forum Istanbul  \n",
       "2           Cash    9/11/2021       Metrocity  \n",
       "3    Credit Card   16/05/2021    Metropol AVM  \n",
       "4           Cash   24/10/2021          Kanyon  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cbe8113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99457, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71257bb9",
   "metadata": {},
   "source": [
    "##### MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e46c8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoice_no        0\n",
      "customer_id       0\n",
      "gender            0\n",
      "age               0\n",
      "category          0\n",
      "quantity          0\n",
      "price             0\n",
      "payment_method    0\n",
      "invoice_date      0\n",
      "shopping_mall     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'rating_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30304\\2965751375.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_missing_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrating_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5900\u001b[0m         ):\n\u001b[0;32m   5901\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5902\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5904\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'rating_count'"
     ]
    }
   ],
   "source": [
    "def check_missing_values(dataframe):\n",
    "    return dataframe.isnull().sum()\n",
    "\n",
    "print(check_missing_values(df))\n",
    "df[df.rating_count.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a30adb1",
   "metadata": {},
   "source": [
    "> Only rating_count present NaN. As it is only two rows I decided to remove then using .dropna() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['rating_count'], inplace=True)\n",
    "print(check_missing_values(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42858343",
   "metadata": {},
   "source": [
    "##### DUPLICATE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be51f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(dataframe):\n",
    "    return dataframe.duplicated().sum()\n",
    "\n",
    "print(check_duplicates(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf3fd57",
   "metadata": {},
   "source": [
    "##### DATA TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_types(dataframe):\n",
    "    return dataframe.dtypes\n",
    "\n",
    "print(check_data_types(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871f5f6",
   "metadata": {},
   "source": [
    "> Some variables in a dataset may have an object data type, which means they are strings. In order to perform numerical analysis on these variables, we need to convert them to numeric values. For example, if we want to calculate the total price of all products, we cannot do so if the price variable is in object format. We need to convert it to a numeric data type first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['discounted_price'] = df['discounted_price'].astype(str).str.replace('₹', '').str.replace(',', '').astype(float)\n",
    "df['actual_price'] = df['actual_price'].astype(str).str.replace('₹', '').str.replace(',', '').astype(float)\n",
    "df['discount_percentage'] = df['discount_percentage'].astype(str).str.replace('%','').astype(float)/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87270a64",
   "metadata": {},
   "source": [
    "> The rating column has a value with an incorrect character, so we will exclude the row to obtain a clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df['rating'].str.contains('\\|').sum()\n",
    "print(f\"Total rows with '|' in 'rating' column: {count}\")\n",
    "df = df[df['rating'].apply(lambda x: '|' not in str(x))]\n",
    "count = df['rating'].str.contains('\\|').sum()\n",
    "print(f\"Total rows with '|' in 'rating' column: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'] = df['rating'].astype(str).str.replace(',', '').astype(float)\n",
    "df['rating_count'] = df['rating_count'].astype(str).str.replace(',', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c826c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_data_types(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412028c",
   "metadata": {},
   "source": [
    "> Now that the data types have been modified, let's add another column to our database that could be useful. \"rating_weighted\" was chosen since it may be used to take into account both the quantity and the average of ratings for a product. This column gives ratings with a high number of raters more weight by dividing the average rating by the total number of ratings. This can assist distinguish between items with high average ratings but few raters and those with great customer satisfaction and numerous positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86520a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_data_types(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f567d",
   "metadata": {},
   "source": [
    "##### DROPING UNUSUFUL COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ddffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['img_link', 'product_link'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d470af",
   "metadata": {},
   "source": [
    "###### CREATING RATING WEIGH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76858fe",
   "metadata": {},
   "source": [
    "> Now that we have adjusted the data types, let's create one more colum that could be interesting to have in our database. \"rating_weighted\" because it can be created as a way of considering not only the average rating, but also the number of people who rated the product. This column weighs the average rating by the number of ratings, giving more weight to ratings with a large number of raters. This can help identify products with high customer satisfaction and many positive ratings compared to products with high average ratings but few raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22376c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the column \"rating_weighted\"\n",
    "df['rating_weighted'] = df['rating'] * df['rating_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640f36e",
   "metadata": {},
   "source": [
    "> In order to have a better understanding of how our items are distributed throughout the various categories I will extract both the main and final categories from the category column. By defining the primary category, we can comprehend the product's categorization in a more general sense, whereas the secondary category explains the product's nature in more detail.\n",
    ">\n",
    "> As an example, if there is a primary category for \"Electronics\" and a subcategory for \"Smartphones\", we may infer that the product in question is a smartphone and so belongs to the electronics category as a whole. Understanding the distribution of our items and spotting trends and patterns within particular categories may both benefit from this knowledge.\n",
    ">\n",
    "> Also, removing both the primary and secondary categories helps improve communication and data presentation. These categories allow us to produce more informative and succinct charts and tables that more clearly depict the distribution of items by category.\n",
    ">\n",
    "> In summary, identifying both the primary and secondary categories is a crucial phase in our exploratory data analysis procedure that aids in our knowledge of the data and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ce86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sub_category'] = df['category'].astype(str).str.split('|').str[-1]\n",
    "df['main_category'] = df['category'].astype(str).str.split('|').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a0c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].str.split('|').apply(lambda x: x[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfa079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category']=df['category'].str.split('|').apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e50be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.set_index('product_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652cff6",
   "metadata": {},
   "source": [
    "##### VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f0eb1",
   "metadata": {},
   "source": [
    "Analyze the distribution of customer ratings using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6517ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "plt.hist(df['rating'])\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.title('Distribution of Customer Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table with values per cluster\n",
    "bins = [0, 1, 2, 3, 4, 5] # Define bin edges\n",
    "df['cluster'] = pd.cut(df['rating'], bins=bins, include_lowest=True, labels=['0-1', '1-2', '2-3', '3-4', '4-5'])\n",
    "table = df['cluster'].value_counts().reset_index().sort_values('index').rename(columns={'index': 'Cluster', 'cluster': 'Number of Reviews'})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa72ca",
   "metadata": {},
   "source": [
    "> The analysis reveals the following insights:\n",
    ">\n",
    ">   * The majority of customer ratings fall within the 3-4 and 4-5 range, comprising a total of 1453 reviews.\n",
    ">   * Notably, there is a rise in reviews within the 2-3 range, compared to the lower 0-1 and 1-2 ranges.\n",
    ">   * The 0-1 range exhibits the fewest reviews, indicating potential areas for enhancing customer satisfaction.\n",
    ">\n",
    "> In summary, while most customers express satisfaction, there are opportunities to further elevate positive ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861de9d",
   "metadata": {},
   "source": [
    "##### CORRELATION MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20106a94",
   "metadata": {},
   "source": [
    "> Perform statistical analysis to identify any correlations between different features, such as the relationship between product price and customer rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7272e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64'])\n",
    "correlation_matrix = numeric_cols.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3397980",
   "metadata": {},
   "source": [
    "##### PLOTTING CORRELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e344473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933438e",
   "metadata": {},
   "source": [
    "> The provided correlation table offers insights into the relationships between various variables in our dataset. Notably, there exists a weak positive correlation between the overall rating and both the rating count and weighted rating, indicating that products with higher ratings tend to garner more reviews and possess higher weighted ratings. Additionally, a moderate positive correlation of 0.121 is observed between the \"rating\" and \"discounted_price\" variables, suggesting that customers may be more inclined to rate a product higher if it is offered at a discounted price. It is important to emphasize that while correlation is identified, causation cannot be inferred. Understanding these associations can, however, provide valuable context for our data. \n",
    ">\n",
    "> To contextualize these findings, it's important to recognize that the strength of a positive correlation—whether weak, moderate, or strong—depends on the specific field of study and research question. A general guideline is as follows: a correlation coefficient (r) between 0.1 and 0.3 denotes a weak positive correlation, while 0.3 to 0.5 indicates a moderate positive correlation. A correlation coefficient surpassing 0.5 signifies a strong positive correlation. Nevertheless, it is crucial to consider factors like sample size, measurement precision, and outliers when interpreting correlation coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4807b5d",
   "metadata": {},
   "source": [
    "##### APPLY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df[['rating', 'rating_count', 'actual_price']]\n",
    "y1 = df['discounted_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b993f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.2)\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X1_train, y1_train)\n",
    "\n",
    "y1_pred = lr_model.predict(X1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016bbcc",
   "metadata": {},
   "source": [
    "##### INTERCEPT, COEFFICIENT, R SQUARED VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a0c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('Linear Regression Intercept: ',lr_model.intercept_)\n",
    "print('Linear Regression Coefficient: ',lr_model.coef_)\n",
    "print('R2 Score: ', r2_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce5726",
   "metadata": {},
   "source": [
    "##### APPLY SCALER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38991493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X2 = df[['rating', 'rating_count', 'actual_price']]\n",
    "y2 = df['discounted_price']\n",
    "\n",
    "X2 = scaler.fit_transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799220f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.2)\n",
    "\n",
    "lr_model.fit(X2_train, y2_train)\n",
    "\n",
    "y2_pred = lr_model.predict(X2_test)\n",
    "print('R2 Score:', r2_score(y2_test, y2_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013bf6f",
   "metadata": {},
   "source": [
    "##### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e04621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X2_train, y2_train)\n",
    "\n",
    "lasso_pred = lasso.predict(X2_test)\n",
    "\n",
    "print('Lasso coefficients: ',lasso.coef_)\n",
    "print('Lasso score: ',lasso.score(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e4d06",
   "metadata": {},
   "source": [
    "##### RIDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e09c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha = 0.1)\n",
    "\n",
    "ridge.fit(X2_train, y2_train)\n",
    "ridge_pred = ridge.predict(X2_test)\n",
    "print('Ridge score: ',ridge.score(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbada8b",
   "metadata": {},
   "source": [
    "##### RECOMENTATION SYSTEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc16cee",
   "metadata": {},
   "source": [
    "> To achieve this goal, I am developing a recommendation system using an Amazon dataset encompassing 1462 rows, with key columns including 'product_name', 'category', 'discounted_price', 'actual_price', among others. Leveraging this dataset, I will build a tailored recommendation engine capable of suggesting products based on a user's browsing history and preferences. This means that upon logging in, users will receive personalized product recommendations, enhancing their shopping experience by introducing them to new and relevant items. This personalized approach aims to make their shopping journey more enjoyable and convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ac8d80",
   "metadata": {},
   "source": [
    "###### 1. USER-BASED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815fb824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['user_id_encoded'] = le.fit_transform(df['user_id'])\n",
    "\n",
    "# Create a new dataframe with the user_id frequency table\n",
    "freq_table = pd.DataFrame({'User ID': df['user_id_encoded'].value_counts().index, 'Frequency': df['user_id_encoded'].value_counts().values})\n",
    "\n",
    "# Display the dataframe\n",
    "print(freq_table)\n",
    "id_example = freq_table.iloc[0,0]\n",
    "print(id_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d63f0",
   "metadata": {},
   "source": [
    "> Converting strings to numbers is a fundamental step in data analysis and machine learning. This transformation is essential for many algorithms that require numeric input to function effectively. It ensures that the algorithm can process the data accurately. Additionally, numerical data offers advantages over strings, enabling mathematical operations for calculations like averages and statistics. In the context of recommendation systems, converting user IDs from strings to numbers streamlines the computation of similarity scores between users or items, enhancing the system's efficiency and accuracy. In summary, this conversion is a crucial and beneficial practice, especially in elevating the performance of recommendation systems within the realm of data analysis and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_products(df, user_id_encoded):\n",
    "    # Use TfidfVectorizer to transform the product descriptions into numerical feature vectors\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    df['about_product'] = df['about_product'].fillna('')  # fill NaN values with empty string\n",
    "    tfidf_matrix = tfidf.fit_transform(df['about_product'])\n",
    "\n",
    "    # Get the purchase history for the user\n",
    "    user_history = df[df['user_id_encoded'] == user_id_encoded]\n",
    "\n",
    "    # Use cosine_similarity to calculate the similarity between each pair of product descriptions\n",
    "    # only for the products that the user has already purchased\n",
    "    indices = user_history.index.tolist()\n",
    "\n",
    "    if indices:\n",
    "        # Create a new similarity matrix with only the rows and columns for the purchased products\n",
    "        cosine_sim_user = cosine_similarity(tfidf_matrix[indices], tfidf_matrix)\n",
    "\n",
    "        # Create a pandas Series with product indices as the index and product names as the values\n",
    "        products = df.iloc[indices]['product_name']\n",
    "        indices = pd.Series(products.index, index=products)\n",
    "\n",
    "        # Get the indices and similarity scores of products similar to the ones the user has already purchased\n",
    "        similarity_scores = list(enumerate(cosine_sim_user[-1]))\n",
    "        similarity_scores = [(i, score) for (i, score) in similarity_scores if i not in indices]\n",
    "\n",
    "        # Sort the similarity scores in descending order\n",
    "        similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get the indices of the top 5 most similar products\n",
    "        top_products = [i[0] for i in similarity_scores[1:6]]\n",
    "\n",
    "        # Get the names of the top 5 most similar products\n",
    "        recommended_products = df.iloc[top_products]['product_name'].tolist()\n",
    "\n",
    "        # Get the reasons for the recommendation\n",
    "        score = [similarity_scores[i][1] for i in range(5)]\n",
    "\n",
    "        # Create a DataFrame with the results\n",
    "        results_df = pd.DataFrame({'Id Encoded': [user_id_encoded] * 5,\n",
    "                                   'recommended product': recommended_products,\n",
    "                                   'score recommendation': score})\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    else:\n",
    "        print(\"No purchase history found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebddebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_products(df, 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d29411",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_products(df, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473e28f",
   "metadata": {},
   "source": [
    "#  (!) INLCUDE COMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b09617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df686f30",
   "metadata": {},
   "source": [
    "   ###### 2. CONTENT-BASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print plot overviews of the first 10 review content.\n",
    "df['review_content'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfIdfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "# Replace NaN with an empty string\n",
    "df['review_content'] = df['review_content'].fillna('')\n",
    "\n",
    "# Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(df['review_content'])\n",
    "\n",
    "# Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0beedb7",
   "metadata": {},
   "source": [
    "> Array mapping from feature integer indices to feature name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e00235",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.get_feature_names_out()[5000:5010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9866460",
   "metadata": {},
   "source": [
    "> From the above output, you observe that 13826 different vocabularies or words in your dataset have 1462 review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d455100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix[:10000], tfidf_matrix[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd727e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba46f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7aea31",
   "metadata": {},
   "source": [
    "##### CALCULATE PRODUCT SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14091f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'product_name' and 'review_content' into a single text column\n",
    "df['text'] = df['product_name'] + ' ' + df['review_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b83d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF vectorization to 'text' column\n",
    "tfidf_matrix = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9235255",
   "metadata": {},
   "source": [
    "##### GET PRODUCT RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_recommendations(product_id, num_recommendations=5):\n",
    "    # Get the index of the product with the given ID\n",
    "    index = df[df['product_id'] == product_id].index[0]\n",
    "\n",
    "    # Get the pairwise similarity scores of the product\n",
    "    sim_scores = list(enumerate(cosine_sim[index]))\n",
    "\n",
    "    # Sort the products based on similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N similar products\n",
    "    top_products = [df.iloc[score[0]] for score in sim_scores[1:num_recommendations+1]]\n",
    "\n",
    "    return top_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8586a",
   "metadata": {},
   "source": [
    "> Now we can replace 'product_id' with any product id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id = 'B08CF3B7N1'  \n",
    "recommendations = get_product_recommendations(product_id)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4e01a",
   "metadata": {},
   "source": [
    "#  (!) INLCUDE COMENT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce86690",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e9c7a",
   "metadata": {},
   "source": [
    "**QUESTION 2**\n",
    "\n",
    "Perform Market Basket Analysis on the chosen dataset by using Apriori and FP growth algorithms. Can\n",
    "you express major divergence between these models? Compare and contrast the machine learning results\n",
    "obtained based on both algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0baef",
   "metadata": {},
   "source": [
    "> **Market basket analysis** - consider products that are frequently reviewed together as being bought together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('amazon.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da3e497",
   "metadata": {},
   "source": [
    "# produto x categoria\n",
    "# categoria x produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d594d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74425111",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948fd442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea8940",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = pd.DataFrame(df[['user_id']].value_counts(), columns=['values'])\n",
    "teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5ab8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a basket for each user\n",
    "basket = (df.groupby(['user_id', 'product_id'])['rating_count']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('user_id'))\n",
    "\n",
    "print(basket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49331a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and declare a method named as 'encode_units()'\n",
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "basket_sets = basket.applymap(encode_units)\n",
    "\n",
    "basket_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5543e4d",
   "metadata": {},
   "source": [
    "# Apriori Rule for Market Basket Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dac77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(basket_sets, min_support = 0.02, use_colnames = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d482352",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric = \"lift\", min_threshold = 5)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc97fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34d924a2",
   "metadata": {},
   "source": [
    "# FP-growth algorithm for Market Basket Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c614ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the frequent item rules for fpgrowth function\n",
    "frequent_itemsets_fp1 = fpgrowth(basket_sets, min_support = 0.1, use_colnames = True)\n",
    "\n",
    "print(frequent_itemsets_fp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba65d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8097a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori algorithm\n",
    "frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)\n",
    "\n",
    "# Print frequent itemsets for inspection\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Association Rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Print rules for inspection\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b43dcc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2e52218",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <p style=\"padding:10px;background-color:#257CCB;margin:0;color:white;font-family:arial;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">_DATA VISUALISATION_</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b4c58",
   "metadata": {},
   "source": [
    "**QUESTION 3**\n",
    "\n",
    "Create an interactive Dashboard aimed at older adults (65+) with specific features to summarise the most\n",
    "important aspects of the data and identify through your visualisation why this dataset is suitable for\n",
    "Machine Learning models in an online retail business. Explain how your dashboard is designed with this\n",
    "demographic in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a5a15",
   "metadata": {},
   "source": [
    "https://www.bing.com/images/search?q=python+dashboard&form=HDRSC3&first=1&cw=1177&ch=689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191907c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('amazon.csv')\n",
    "\n",
    "# Visualization 1: Bar chart for product categories\n",
    "category_counts = df['category'].value_counts().reset_index()\n",
    "category_counts.columns = ['Category', 'Count']\n",
    "\n",
    "fig1 = px.bar(category_counts, x='Category', y='Count', title='Product Categories Distribution')\n",
    "fig1.show()\n",
    "\n",
    "# Visualization 2: Line chart for sales trends\n",
    "sales_trends = df.groupby('product_name')['rating_count'].sum().reset_index().sort_values(by='rating_count', ascending=False)[:10]\n",
    "\n",
    "fig2 = px.line(sales_trends, x='product_name', y='rating_count', title='Top 10 Products by Rating Count')\n",
    "fig2.show()\n",
    "\n",
    "# Visualization 3: Pie chart for product ratings distribution\n",
    "rating_distribution = df['rating'].value_counts().reset_index()\n",
    "rating_distribution.columns = ['Rating', 'Count']\n",
    "\n",
    "fig3 = px.pie(rating_distribution, values='Count', names='Rating', title='Product Ratings Distribution')\n",
    "fig3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_data.csv' with the actual file path or URL of your Amazon dataset\n",
    "amazon_data = pd.read_csv('amazon.csv')\n",
    "\n",
    "# Perform any necessary data preprocessing\n",
    "# For example, aggregate sales data by month\n",
    "discount_data = amazon_data.groupby('category')['discount_percentage'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc413de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc, html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dashboard app\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "# Define the layout of the dashboard\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Amazon Online Retail Dashboard\"),\n",
    "    \n",
    "    # Sales Trends Line Chart\n",
    "    dcc.Graph(\n",
    "        id='sales-trends',\n",
    "        figure=px.line(discount_data, x='category', y='discount_percentage', title='Monthly Sales Trends')\n",
    "    ),\n",
    "\n",
    "    # Product Category Pie Chart\n",
    "    dcc.Graph(\n",
    "        id='product-categories',\n",
    "        figure=px.pie(amazon_data, names='product_id', title='Product Category Distribution')\n",
    "    ),\n",
    "\n",
    "    # Customer Reviews Sentiment Heatmap\n",
    "    # (Note: This is a simplified example, and sentiment analysis requires additional processing)\n",
    "    dcc.Graph(\n",
    "        id='sentiment-heatmap',\n",
    "        figure=px.imshow([[0.8, 0.2], [0.4, 0.6]], color_continuous_scale='Viridis', title='Customer Reviews Sentiment')\n",
    "    )\n",
    "])\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db7b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8304eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ad21c16",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <p style=\"padding:10px;background-color:#257CCB;margin:0;color:white;font-family:arial;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">_TITLE HERE_</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014bcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd4cc257",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <p style=\"padding:10px;background-color:#257CCB;margin:0;color:white;font-family:arial;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">_REFERENCES_</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d7a5b0",
   "metadata": {},
   "source": [
    "(22/10)\n",
    " \n",
    "https://stackoverflow.com/questions/14088687/how-to-change-plot-background-color\n",
    "    \n",
    "https://datascientyst.com/full-list-named-colors-pandas-python-matplotlib/\n",
    "\n",
    "https://htmlcolorcodes.com/\n",
    "\n",
    "Amazon Customer Reviews (dataset) https://www.tensorflow.org/datasets/catalog/amazon_us_reviews\n",
    "\n",
    "https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/datasets/amazon_us_reviews/dummy_data/test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1bdde2",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "Linden, G., Smith, B., & York, J. (2003). Amazon.com recommendations: Item-to-item collaborative filtering. IEEE Internet Computing, 7(1), 76-80.\n",
    "\n",
    "Yin, D., Hong, L., Davison, B. D., & Sidiropoulos, N. (2012). Comparative recommendation: A study of effectiveness metrics. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval (pp. 339-348).\n",
    "\n",
    "McFee, B., Lanckriet, G. R., & Lanckriet, G. R. (2012). Metric learning to rank. In Advances in neural information processing systems (pp. 2573-2581).\n",
    "\n",
    "Kang, J., McAuley, J., & Leskovec, J. (2012). Exploring patterns of activity on e-commerce sites. In Proceedings of the 21st international conference on World Wide Web (pp. 795-804)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5348a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
